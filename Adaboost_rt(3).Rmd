---
title: "Adaboost_rt"
output: html_document
date: "2023-12-20"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
options(timeout=300)
library(rpart)
library(tidyr)
library(tidyverse)
# install.packages("adabag", dependencies = TRUE)
library(adabag)
library(caret)
library(palmerpenguins)
data("penguins")
```

```{r}
adaboost_rt <- function(data, outcome_var, M) {
  # data: predictor variables or features of the dataset
  # outcome_var: response variable or target variable of the dataset
  # M: the number of decision trees included in the ensemble
  
  y <- data[[outcome_var]]
  n <- length(y)
  weights <- rep(1/n, n)
  models <- list()

  for (m in 1:M) {
    model <- rpart(formula = as.formula(paste(outcome_variable, "~ .")), data = data, weights = weights)

    #model <- rpart(y ~ ., data = data, weights = weights)
    predictions <- predict(model, data)
    
    # Calculate Loss function L
    diff <- abs(y - predictions)
    L <- diff/max(diff)
    epsilon <- sum(weights * L)
    
    # Calculate beta to update the model weight 
    beta <- epsilon/(1 - epsilon) 
    
    # Update the weights
    weights <- weights * ( beta ^ (1-L) )

    # Normalize the weights
    weights <- weights / sum(weights)

    models[[m]] <- list(model)
  }

  return(models)
}
```

```{r}
# Preprocess Penguins data
penguins <- penguins %>% drop_na() 

# Split data to test and train
set.seed(123)  
indices <- sample(1:nrow(penguins), 0.7 * nrow(penguins))
train_data <- penguins[indices, ]
test_data <- penguins[-indices, ]

# Test the Adaboost model
outcome_variable <- "sex"
models <- adaboost_rt(train_data, outcome_variable, 100)
```

```{r}
# compare with standard function (adabag)
predictions1 <- predict(models, test_data)
observed1 <- test_data[[outcome_variable]]
rmse_ada <- c()
for(m in 1:length(models)){
  rmse_ada[m] <- sqrt(mean((observed1 - predictions1[[m]][[1]]) ^ 2))
}
rmse1 <- rmse_ada[length(models)]
```

```{r}
# compare with adabag function (without cv)
ada_model <- boosting(species ~ ., data = train_data, mfinal = 10)
predictions <- predict(ada_model, newdata = test_data)
confusion_matrix <- table(test_data$species, predictions$class)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
accuracy
```
```{r}
# compare with adabag function (with cv)
cvmodel = boosting.cv(species ~., data = penguins, boos = TRUE, mfinal = 10, v =5)
cvmodel$confusion
```

```{r}
adaboost_cv <- function(data, outcome_var, M, folds = 5) {
  set.seed(123)
  y <- data[[outcome_var]]
  n <- nrow(data)
  size <- floor(n / folds)
  indices <- sample(1:n)
  mse1 <- c()
  mse <- c()
  
  for (i in 1:folds) {
    # Split data into training and validation sets
    validation_indices <- indices[((i-1) * size + 1):(i * size)]
    train_indices <- setdiff(indices, validation_indices)
    
    train_data <- data[train_indices, ]
    validation_data <- data[validation_indices, ]
    
    # Train AdaBoost model 
    models <- adaboost_rt(train_data, outcome_var, M)

    # Make predictions
    predictions <- predict(models, validation_data)
    
    for(m in 1:length(models)){
      mse1[m] <- mean((validation_data[[outcome_var]] - predictions[[m]][[1]]) ^ 2)
    }
    
    mse[i] <- mse1[length(models)]
  }
  return(mean(mse))
}


# Tune hyperparameter M
M_values <- c(5, 10, 15, 20, 30, 50, 70)
cv_results <- c()
for(m in 1:length(M_values)){
  cv_results[m] <- adaboost_cv(train_data, outcome_variable, M=M_values[m], folds = 5)
}

best_M <- M_values[which.min(cv_results)]
cat("Best M:", best_M, "\n", cv_results, "\n")

```




