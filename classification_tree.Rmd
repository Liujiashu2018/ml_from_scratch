---
title: "Classification Trees"
output: html_document
date: "2023-11-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(palmerpenguins)
library(tidyverse)
library(dplyr)
library(stringr)
data("penguins")
# Classification question: species classification
# Predict the species of the penguin (e.g., Adelie, Chinstrap, Gentoo) based on other variables such as bill length, bill depth, flipper length, body mass, island, and sex.
```

```{r}
# Preprocessing Data
penguins <- na.omit(penguins)
penguins <- penguins %>% 
  select(-year)
#penguins$island <- as.numeric(as.factor(penguins$island))
#penguins$sex <- as.numeric(as.factor(penguins$sex))
```

Thoughts on building a simple classification tree:

1. Define a function for gini_impurity
2. Loop over all the predictors and possible split points. Pick the predictor that result in the lowest weighted average Gini impurity.

Note: for numeric predictors, we should calculate the average for all adjacent penguins, and then calcualte the Gini impurity for each average numeric value.
Choose the threshold that has the lowest weighted total Gini impurity

3. Starting from the previously formed regions, repeat step 2. 
4. Stop once no possible split can lower a node's Gini impurity. 

```{r}
numeric_features <- names(penguins)[sapply(penguins, is.numeric)]
categorical_features <- names(penguins)[sapply(penguins, is.factor)] 

print("Numeric Features:")
print(numeric_features)

print("Categorical Features:")
print(categorical_features)
```
```{r}
logical_vector <-penguins[['bill_length_mm']]<= 38
print(logical_vector)
```


```{r}
# Calculate Gini Index (Classification)
gini_impurity <- function(y){
  # assumes y if a factor with all levels
  if(length(y) == 0) return(0)
  p <- table(y)/length(y)
  1-sum(p^2)
}
```

```{r}
# Best Split 
best_split <- function(data, target){
  best_split <- NULL
  lowest_impurity <- Inf
  best_var <- NULL
  best_threshold <- NULL
  
  # Loop through each predictor
  for (var in names(data)){
    if (var == target) next #skip the target variable
    # separate handling for numeric and categorical predictors
    if (is.numeric(data[[var]])) {
      # for numeric predictors
      values <- sort(unique(data[[var]]))
      thresholds <- (values[-length(values)] + values[-1]) / 2 
      # values[-length(values)] --> excluding the last element of the vector because there's no "next value" to pair it with for calculating an average
      # values[-1] --> takes all elements of the values vector except the first one
    }else {
      # for categorical predictors
      thresholds <- unique(data[[var]])
    }
    for (threshold in thresholds) {
      if (is.numeric(data[[var]])) {
        left_split <- data[data[[var]] <= threshold, ]
        right_split <- data[data[[var]] > threshold, ]
      } else {
        left_split <- data[data[[var]] == threshold, ]
        right_split <- data[data[[var]] != threshold, ]}
      
      # calculate weighted Gini impurity
      left_impurity <- gini_impurity(left_split[[target]])
      right_impurity <- gini_impurity(right_split[[target]])
      total_impurity <- (nrow(left_split) * left_impurity + nrow(right_split) * right_impurity) / nrow(data)
      
      # update best split if this is the lowest impurity found so far
      if (total_impurity < lowest_impurity) {
        lowest_impurity <- total_impurity
        best_split <- list(variable = var, threshold = threshold)
        best_var <- var
        best_threshold <- threshold
      }
    }
  }
  return(best_split)
}
```

```{r}
split <- best_split(penguins, "species")
print(split)
```
```{r}
# Create a simple tree
create_tree <- function(data, target, max_depth = Inf, min_split = 1, current_depth = 1) {
  
  # Check stopping criteria
  if (nrow(data) < min_split || current_depth >= max_depth || length(unique(data[[target]])) == 1) {
    return(table(data[[target]])[1])
  }
  
  # Find the best split
  split <- best_split(data, target)
  
  # If no further split can be made, return the most common class
  if (is.null(split)) {
    return(table(data[[target]])[1])
  }

  # Recursively apply to left and right splits
  # Handle splitting for numeric and categorical variables
  if (is.numeric(data[[split$variable]])) {
    left_split <- data[data[[split$variable]] <= split$threshold, ]
    right_split <- data[data[[split$variable]] > split$threshold, ]
  } else {
    left_split <- data[data[[split$variable]] == split$threshold, ]
    right_split <- data[data[[split$variable]] != split$threshold, ]}
  
  left_tree <- create_tree(left_split, target, max_depth, min_split, current_depth + 1)
  right_tree <- create_tree(right_split, target, max_depth, min_split, current_depth + 1)
  
  # Return a list representing the tree node
  return(list(variable = split$variable, threshold = split$threshold, left = left_tree, right = right_tree))
}
```

```{r}
predict_instance <- function(node, instance) {
  if(is.null(node$left) && is.null(node$right)) {
    # Leaf node
    return(node$prediction)
  }

  # Check the feature and threshold to decide whether to go left or right
  if(is.numeric(node$variable)) {
    if(instance[node$variable] <= node$threshold) {
      return(predict_instance(node$left, instance))
    } else {
      return(predict_instance(node$right, instance))
    }
  } else {
    if(instance[node$variable] == node$threshold) {
      return(predict_instance(node$left, instance))
    } else {
      return(predict_instance(node$right, instance))
    }
  }
}
```

```{r}
set.seed(123) 
sample_size <- floor(0.8 * nrow(penguins))
train_indices <- sample(seq_len(nrow(penguins)), size = sample_size)
train_data <- penguins[train_indices, ]
test_data <- penguins[-train_indices, ]

target_variable <- "species"
```

```{r}
my_tree <- build_tree(train_data, target_variable)
# Apply the prediction function to each row in the test set
test_predictions <- sapply(1:nrow(test_data), function(i) {
  predict_instance(my_tree, test_data[i, ])
})
```
```{r}
# Assuming test_data contains the actual labels
accuracy <- sum(test_predictions == test_data[[target_variable]]) / nrow(test_data)
print(accuracy)
```








Can you help me build the rest of the recursive splitting part? My thought is, after finding the best split, I can then split the data into two side: left and right. And the recursive process can be applied to each of the subset
Recursive splitting missing...

```{r}
# Recursive function to build the tree
build_tree <- function(data, target, current_depth=0, max_depth=Inf, min_size=1) {
  
  # Base cases
  if(nrow(data) <= min_size || current_depth >= max_depth || length(unique(data[[target]])) == 1) {
    return(list(prediction=which.max(table(data[[target]]))))
  }
  
  # Recursive case
  split <- best_split(data, target)
  if(is.null(split)) {
    return(list(prediction=which.max(table(data[[target]]))))
  }
  
  left_data <- data[data[[split$variable]] <= split$threshold, ]
  right_data <- data[data[[split$variable]] > split$threshold, ]
  
  node <- list(
    variable=split$variable,
    threshold=split$threshold,
    left=build_tree(left_data, target, current_depth+1, max_depth, min_size),
    right=build_tree(right_data, target, current_depth+1, max_depth, min_size)
  )
  
  return(node)
}
```

```{r}
predict_instance <- function(node, instance) {
  # Check if we are at a leaf node
  if(is.null(node$left) && is.null(node$right)) {
    return(node$prediction)
  }

  # Check for missing values or non-existent attributes
  if(is.na(instance[node$variable]) || !node$variable %in% names(instance)) {
    # Return a default prediction or handle the missing value case
    # This might be the most common class, or you could return NA or handle it some other way
    return(node$prediction)  # Adjust this as needed
  }

  # Determine if the variable is categorical or numerical
  if(is.factor(instance[node$variable]) || is.character(instance[node$variable])) {
    # For categorical variables, check if it matches the threshold (category)
    if(instance[node$variable] == node$threshold) {
      return(predict_instance(node$left, instance))
    } else {
      return(predict_instance(node$right, instance))
    }
  } else {
    # For numerical variables, check if it's less than or equal to the threshold
    if(instance[node$variable] <= node$threshold) {
      return(predict_instance(node$left, instance))
    } else {
      return(predict_instance(node$right, instance))
    }
  }
}

```

```{r}
set.seed(123) 
sample_size <- floor(0.8 * nrow(penguins))
train_indices <- sample(seq_len(nrow(penguins)), size = sample_size)
train_data <- penguins[train_indices, ]
test_data <- penguins[-train_indices, ]

target_variable <- "species"
```
```{r}
my_tree <- build_tree(train_data, target_variable)
predictions <- predict_tree(my_tree, test_data)
```

```{r}
print(my_tree)
```



