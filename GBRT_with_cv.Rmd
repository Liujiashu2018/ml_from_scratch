---
title: "Untitled"
author: "Jiashu Liu & Guanhua Xu"
date: "2023-12-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(rpart)
library(tidyr)
library(tidyverse)
library(gbm)
library(palmerpenguins)
library(caret)
data("penguins")
```


```{r}
# Split data to test and train
penguins <- penguins %>% drop_na()
set.seed(123)  
indices <- sample(1:nrow(penguins), 0.7 * nrow(penguins))
train_data <- penguins[indices, ]
test_data <- penguins[-indices, ]
target <- "body_mass_g"
```

```{r}
# Simple GBRT
gbrt <- function(train_data,test_data, target, learning_rate = 0.1, n_trees = 100) {
  models <- list() # store the sequence of the trees that will be created
  residuals <- train_data[[target]] # Initially is just the target value, but later will be updated to (observed - predicted)
  
  # Iterate n_trees times to build the sequence of trees
  for (i in 1:n_trees) {
    tree <- rpart(residuals ~ ., data = train_data, method = "anova") # use anova for regression problem 
    predictions <- predict(tree, train_data) 
    
    # The most crucial step in GBRT! Update the previous residual by subtracting the scaled predictions
    residuals <- residuals - learning_rate * predictions
    
    # Update the target col in the dataset to new residuals. In the next iteration, this will be the new response variable. 
    train_data[[target]] <- residuals
    models[[i]] <- tree
  }
    # make predictions
    predictions <- rep(0, nrow(test_data))
    for (tree in models) {
    predictions <- predictions + learning_rate * predict(tree, test_data)
  }
  
    return(predictions)
}
```

```{r}
# Evaluate model performance using RMSE
predictions <- gbrt(train_data,test_data,target)
rmse <- function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
rmse_custom <- rmse(test_data[[target]], predictions)
print(paste("RMSE Custom GBRT:", rmse_custom))#"RMSE Custom GBRT: 1675.89703022131"
```

Testing penguins using existing gbm package in R
```{r}
# Train the model
gbm_model <- gbm(
  formula = as.formula(paste(target, "~ .")),
  distribution = "gaussian",
  data = train_data,
  n.trees = 100,
  cv.folds = 5,  # for cross-validation
  verbose = FALSE
)

# Make predictions
gbm_predictions <- predict(gbm_model, test_data)
# gbm rmse
rmse_gbm <- rmse(test_data[[target]], gbm_predictions)
print(paste("RMSE GBM:", rmse_gbm))#"RMSE GBM: 300.428501171076"
```


```{r}
# Define the cross-validation function for tuning the learning rate
gbrt_cv <- function(data, target, learning_rates, n_trees = 100, folds = 5) {
  set.seed(123)  
  
  # Split data to test and train
  indices <- sample(1:nrow(data), 0.7 * nrow(data))
  train_data <- data[indices, ]
  test_data <- data[-indices, ]
  
  # Initialize results data frame
  cv_results <- data.frame(learning_rate = numeric(), rmse = numeric())
  
  for (lr in learning_rates) {
    # Perform k-fold cross-validation
    folds_indices <- createFolds(train_data[[target]], k = folds, list = TRUE, returnTrain = FALSE)
    rmse_values <- numeric()
    
    for (fold in folds_indices) {
      train_indices <- unlist(fold)
      cv_train_data <- train_data[train_indices, ]
      cv_test_data <- train_data[-train_indices, ]
      
      # Train GBRT
      models <- gbrt(cv_train_data, target, learning_rate = lr, n_trees = n_trees)
      
      # Make predictions
      predictions <- predict_gbrt(models, cv_test_data, learning_rate = lr)
      
      # Evaluate RMSE
     rmse <- function(actual, predicted) {
        sqrt(mean((actual - predicted)^2))
      }
      rmse_value <- rmse(cv_test_data[[target]], predictions)
      rmse_values <- c(rmse_values, rmse_value)
    }
    
    # Calculate average RMSE across folds
    avg_rmse <- mean(rmse_values)
    
    cv_results <- rbind(cv_results, data.frame(learning_rate = lr, rmse = avg_rmse))
  }
  
  # Find the learning rate with the minimum RMSE
  best_lr <- cv_results$learning_rate[which.min(cv_results$rmse)]
  
  print(cv_results)
  cat("Best Learning Rate:", best_lr, "\n")
  
  return(cv_results)
}

# Define the learning rates to tune
learning_rates <- c(0.01, 0.05, 0.1, 0.2, 0.5)
cv_results <- gbrt_cv(penguins, target = "body_mass_g", learning_rates = learning_rates)
```
