---
title: "KNN with Cross Validation"
author: "Jiashu Liu"
date: "2023-12-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Code KNN algorithm from scratch 

1. KNN is a non-parametric algorithm.
2. Thoughts on coding KNN:
- Different distance measures (most commonly used: Euclidean)
- Choose the number of k nears neighbors
- Make predictions from the algorithm 

```{r}
library(tidyr)
library(dplyr)
library(tidyverse)
library(class)
```
## Euclidean Distance: 
Distance between points $x_i$ and $x_j$ in p-dimentional space. It measures the diagonal distance between the two points.
$$d(\vec{x_i}, \vec{x_j}) = \sqrt{(x_{i1}+x_{j1})^2+ (x_{i2}+x_{j2})^2 + ...+(x_{ip}+x_{jp})}$$
```{r}
euclidean_distance = function(a, b){
  #  We check that they have the same number of observation
  if(length(a) == length(b)){
    sqrt(sum((a-b)^2))  
  } else{
    stop('Vectors must be of the same length')
  }
}
```

Manhattan Distance
$$ Mdis = |x_2 – x_1| + |y_2 – y_1|$$
```{r}
manhattan_distance = function(a, b){
  #  We check that they have the same number of observation
  if(length(a) == length(b)){
    sum(abs(a-b))
  } else{
    stop('Vectors must be of the same length')
  }
}
```

Minkowski Distance:
p = 1: Manhattan distance
p = 2: Euclidean distance
1 and 2 are the most commonly used value for Minkowski Distance, but it can also take other values. 
$$ Minkowski = \Bigg(\sum^d_{l=1}|x_{il}-x_{jl}|^{1/p}\Bigg)^p$$
```{r}
minkowski_distance = function(a,b,p){
  if(p<=0){
   stop('p must be higher than 0') 
  }

  if(length(a)== length(b)){
    sum(abs(a-b)^p)^(1/p)
  }else{
     stop('Vectors must be of the same length')

  }
}
```

```{r}
knn <- function(train_data, train_labels, test_data, k, distance_function) {
  # Helper function to find the most common label
  most_common_label <- function(labels) {
    return(names(sort(table(labels), decreasing = TRUE)[1]))
  }

  # Applying the KNN algorithm
  predictions <- sapply(1:nrow(test_data), function(i) {
    # Calculate distances between the test point and all training points
    distances <- sapply(1:nrow(train_data), function(j) {
      distance_function(test_data[i, ], train_data[j, ])
    })
    
    # Combine distances with labels and sort them
    neighbors <- data.frame(Distance = distances, Label = train_labels) %>%
                 arrange(Distance) %>%
                 head(k)
    
    # Return the most common label among the neighbors
    return(most_common_label(neighbors$Label))
  })

  return(predictions)
}
```

```{r}
library(palmerpenguins)
data("penguins")
# Preprocess penguins data
penguins <- penguins %>% 
  na.omit() %>% 
  select(-year)

features <- penguins[, c("bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g")]
target <- penguins$species
```

```{r}
set.seed(123) 

# Sample indices for splitting the data
indices <- sample(1:nrow(features), size = 0.7 * nrow(features))

# Create training and test sets
train_x <- features[indices, ]
train_y <- target[indices]
test_x <- features[-indices, ]
test_y <- target[-indices]

# Test the customized algorithm
k <- 5
predictions <- knn(train_x, train_y, test_x, k, euclidean_distance)

# Calculate accuracy
accuracy <- sum(predictions == test_y) / length(test_y)
print(paste("Accuracy:", accuracy))
```

```{r}
# KNN with cross validation
knn_cv <- function(data, target, k_folds, k_neighbors, distance_function) {
  set.seed(123) 
  shuffled_data <- data[sample(nrow(data)), ]
  
  # Split data into k folds
  folds <- split(shuffled_data, cut(seq(1, nrow(shuffled_data)), breaks = k_folds, labels = FALSE))

  # Define a function to calculate accuracy
  calculate_accuracy <- function(predictions, test_labels) {
    sum(predictions == test_labels) / length(test_labels)
  }
  
  accuracies <- numeric(k_folds)
  
  # Cross-validation
  for(i in 1:k_folds) {
    # Splitting the data into training and test sets
    test_data <- folds[[i]]
    train_data <- do.call("rbind", folds[-i])
    
    # Extract labels
    train_labels <- train_data[[target]]
    test_labels <- test_data[[target]]
    
    # Remove labels from features
    train_data <- train_data[, !names(train_data) %in% target]
    test_data <- test_data[, !names(test_data) %in% target]
    
    predictions <- knn(train_data, train_labels, test_data, k_neighbors, distance_function)
    accuracies[i] <- calculate_accuracy(predictions, test_labels)
  }
  
  return(accuracies)
}
```

```{r}
k_folds <- 5
k_neighbors <- 5
cv_results <- knn_cv(penguins, "species", k_folds, k_neighbors, euclidean_distance)

# Calculating the average accuracy
average_accuracy <- mean(cv_results)
print(paste("Average accuracy after cross validation:", average_accuracy))
```

Compute KNN with Class package: 
1. Assign sel to be the indices in the training set, which should be 70% of the data
2. Split the data into train and test based on sel (only include columns c(1,3:6))
3. Run knn.cv on the train data with k=1,2,…,9 (we aren’t worried about ties);
4. Evaluate the accuracy of the train set and choose the k based on it
5. Rerun knn with the k chosen, with train and test. 
6. Evaluate the accuracy of the test set (report confusion matrix as well).

```{r}
set.seed(123)
n <- nrow(penguins)
m <- round(0.70*n)
sel <- sample(1:n,m,replace=F)
train <- penguins[sel,c(1,3:6)]
test <- penguins[-sel,c(1,3:6)]
fit <- vector("list",9)
acc <- vector("numeric",9)
for (k in 1:9) {
  fit[[k]] <- knn.cv(train[,-1],cl=train$species,k=k)
  acc[k] <- confusionMatrix(fit[[k]],train$species)$overall["Accuracy"]
}
plot(acc,type='l')
max(acc)
k <- which.max(acc)
refit <- class::knn(train[,-1],test[,-1],cl=train$species,k=k)
confusionMatrix(refit,test$species)
```


